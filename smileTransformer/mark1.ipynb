{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Import and process data\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "df = pd.read_csv('../data/chembl_24_chemreps.csv')\r\n",
    "\r\n",
    "lengths = list(map(len, df['canonical_smiles'].values))\r\n",
    "\r\n",
    "print('Total smiles', len(df))\r\n",
    "df_sub = df[np.array(lengths)<=100]\r\n",
    "print('After filteration', len(df_sub))\r\n",
    "lengths2 = list(map(len, df_sub['canonical_smiles'].values))\r\n",
    "print('Max length', np.max(lengths2))\r\n",
    "\r\n",
    "df_sub.to_csv('../data/chembl_24_100.csv')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total smiles 1048575\n",
      "After filteration 997329\n",
      "Max length 100\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Build corpus\r\n",
    "!python build_corpus.py -i ../data/chembl_24_100.csv -o ../data/processed/smiles_corpus_24_100.csv \r\n",
    "# Build Vocab\r\n",
    "!python build_vocab.py -c ../data/processed/smiles_corpus_24_100.csv -o ../data/processed/vocab_24_100.pkl  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\r\n",
    "from pretrain_trfm import TrfmSeq2seq\r\n",
    "from pretrain_rnn import RNNSeq2Seq\r\n",
    "#from bert import BERT\r\n",
    "from build_vocab import WordVocab\r\n",
    "from utils import split\r\n",
    "\r\n",
    "pad_index = 0\r\n",
    "unk_index = 1\r\n",
    "eos_index = 2\r\n",
    "sos_index = 3\r\n",
    "mask_index = 4\r\n",
    "\r\n",
    "vocab = WordVocab.load_vocab('/home/yogesh/data/dnndr2/processedData/vocab_24.pkl')\r\n",
    "\r\n",
    "trfm = TrfmSeq2seq(len(vocab), 256, len(vocab), 3)\r\n",
    "trfm.load_state_dict(torch.load('/home/yogesh/data/dnndr2/pretrained_ST/trfm_12_23000.pkl'), strict=False)\r\n",
    "trfm.eval()\r\n",
    "print('Total parameters:', sum(p.numel() for p in trfm.parameters()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"build_corpus.py\", line 5, in <module>\n",
      "    from utils import split\n",
      "  File \"f:\\Projects\\202107_dnndr2\\smileTransformer\\utils.py\", line 1, in <module>\n",
      "    import torch\n",
      "ModuleNotFoundError: No module named 'torch'\n",
      "Traceback (most recent call last):\n",
      "  File \"build_vocab.py\", line 171, in <module>\n",
      "    main()\n",
      "  File \"build_vocab.py\", line 164, in main\n",
      "    with open(args.corpus_path, \"r\", encoding=args.encoding) as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/yogesh/data/dnndr2/processedData/smiles_corpus_24_100.csv'\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8ffeeaeaf9ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'python build_vocab.py -c /home/yogesh/data/dnndr2/processedData/smiles_corpus_24_100.csv -o /home/yogesh/data/dnndr2/processedData/vocab_24_100.pkl  \\r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpretrain_trfm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTrfmSeq2seq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpretrain_rnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRNNSeq2Seq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Build corpus\r\n",
    "!python build_corpus.py -i /home/yogesh/data/dnndr2/rawData/drug_smiles.csv -o /home/yogesh/data/dnndr2/processedData/smiles_corpus.csv "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "100%|██████████████████████████████████| 30654/30654 [00:00<00:00, 34122.52it/s]\n",
      "Built a corpus file!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Build corpus\r\n",
    "!python build_corpus.py -i /home/yogesh/data/dnndr2/rawData/chembl_24_chemreps_100.csv -o /home/yogesh/data/dnndr2/processedData/smiles_corpus_24_100.csv "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "100%|████████████████████████████████| 997329/997329 [00:29<00:00, 33870.05it/s]\n",
      "Built a corpus file!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Build Vocab\r\n",
    "!python build_vocab.py -c /home/yogesh/data/dnndr2/processedData/smiles_corpus_24_100.csv -o /home/yogesh/data/dnndr2/processedData/vocab_24_100.pkl  "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Building Vocab\n",
      "VOCAB SIZE: 45\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "df = pd.read_csv('../data/chembl_24_chemreps.csv')\r\n",
    "\r\n",
    "lengths = list(map(len, df['canonical_smiles'].values))\r\n",
    "\r\n",
    "plt.hist(lengths, bins=100)\r\n",
    "plt.grid()\r\n",
    "plt.xlabel('SMILES length')\r\n",
    "plt.ylabel('Counts')\r\n",
    "plt.yscale('log')\r\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_sub.to_csv('chembl_24_chemreps_100.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!python pretrain_trfm.py -v /home/yogesh/data/dnndr2/processedData/vocab_24_100.pkl -d /home/yogesh/data/dnndr2/rawData/chembl_24_chemreps_100.csv -o /home/yogesh/data/dnndr2/results/"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading dataset...\n",
      "Train size: 987329\n",
      "Test size: 10000\n",
      "TrfmSeq2seq(\n",
      "  (embed): Embedding(45, 256)\n",
      "  (pe): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (trfm): Transformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (3): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (3): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (out): Linear(in_features=256, out_features=45, bias=True)\n",
      ")\n",
      "Total parameters: 4245037\n",
      "0it [00:00, ?it/s]Train   1: iter     0 | loss 4.029 | ppl 56.187\n",
      "Val   1: iter     0 | loss 3.279 | ppl 26.542\n",
      "[!] saving model...\n",
      "998it [00:40, 33.04it/s]Train   1: iter  1000 | loss 0.002 | ppl 1.002\n",
      "1998it [01:10, 35.20it/s]Train   1: iter  2000 | loss 0.001 | ppl 1.001\n",
      "2998it [01:41, 26.32it/s]Train   1: iter  3000 | loss 0.000 | ppl 1.000\n",
      "3999it [02:13, 30.65it/s]Train   1: iter  4000 | loss 0.000 | ppl 1.000\n",
      "4998it [02:42, 37.99it/s]Train   1: iter  5000 | loss 0.000 | ppl 1.000\n",
      "5997it [03:12, 30.78it/s]Train   1: iter  6000 | loss 0.000 | ppl 1.000\n",
      "6997it [03:44, 30.34it/s]Train   1: iter  7000 | loss 0.000 | ppl 1.000\n",
      "7998it [04:15, 34.81it/s]Train   1: iter  8000 | loss 0.000 | ppl 1.000\n",
      "8998it [04:43, 28.85it/s]Train   1: iter  9000 | loss 0.000 | ppl 1.000\n",
      "9999it [05:13, 35.82it/s]Train   1: iter 10000 | loss 0.000 | ppl 1.000\n",
      "Val   1: iter 10000 | loss 0.000 | ppl 1.000\n",
      "[!] saving model...\n",
      "10998it [05:54, 30.40it/s]Train   1: iter 11000 | loss 0.000 | ppl 1.000\n",
      "11998it [06:25, 34.94it/s]Train   1: iter 12000 | loss 0.000 | ppl 1.000\n",
      "12998it [06:56, 32.84it/s]Train   1: iter 13000 | loss 0.000 | ppl 1.000\n",
      "13998it [07:26, 28.40it/s]Train   1: iter 14000 | loss 0.000 | ppl 1.000\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "14999it [07:57, 33.10it/s]Train   1: iter 15000 | loss 0.000 | ppl 1.000\n",
      "15997it [08:28, 27.60it/s]Train   1: iter 16000 | loss 0.000 | ppl 1.000\n",
      "16998it [08:59, 40.23it/s]Train   1: iter 17000 | loss 0.000 | ppl 1.000\n",
      "17996it [09:30, 38.00it/s]Train   1: iter 18000 | loss 0.000 | ppl 1.000\n",
      "18999it [10:03, 32.02it/s]Train   1: iter 19000 | loss 0.000 | ppl 1.000\n",
      "19997it [10:40, 24.56it/s]Train   1: iter 20000 | loss 0.000 | ppl 1.000\n",
      "Val   1: iter 20000 | loss 0.000 | ppl 1.000\n",
      "[!] saving model...\n",
      "20998it [11:34, 21.97it/s]Train   1: iter 21000 | loss 0.000 | ppl 1.000\n",
      "21998it [12:08, 35.22it/s]Train   1: iter 22000 | loss 0.000 | ppl 1.000\n",
      "22999it [12:38, 40.10it/s]Train   1: iter 23000 | loss 0.000 | ppl 1.000\n",
      "23999it [13:06, 34.61it/s]Train   1: iter 24000 | loss 0.000 | ppl 1.000\n",
      "24997it [13:35, 32.52it/s]Train   1: iter 25000 | loss 0.000 | ppl 1.000\n",
      "26000it [14:01, 45.28it/s]Train   1: iter 26000 | loss 0.000 | ppl 1.000\n",
      "26999it [14:24, 45.93it/s]Train   1: iter 27000 | loss 0.000 | ppl 1.000\n",
      "27997it [14:48, 37.00it/s]Train   1: iter 28000 | loss 0.000 | ppl 1.000\n",
      "28999it [15:12, 40.88it/s]Train   1: iter 29000 | loss 0.000 | ppl 1.000\n",
      "29997it [15:38, 39.81it/s]Train   1: iter 30000 | loss 0.000 | ppl 1.000\n",
      "Val   1: iter 30000 | loss 0.000 | ppl 1.000\n",
      "[!] saving model...\n",
      "31000it [16:18, 32.07it/s]Train   1: iter 31000 | loss 0.000 | ppl 1.000\n",
      "31999it [16:49, 32.49it/s]Train   1: iter 32000 | loss 0.000 | ppl 1.000\n",
      "32998it [17:17, 30.86it/s]Train   1: iter 33000 | loss 0.000 | ppl 1.000\n",
      "33998it [17:47, 28.70it/s]Train   1: iter 34000 | loss 0.000 | ppl 1.000\n",
      "35000it [18:16, 36.63it/s]Train   1: iter 35000 | loss 0.000 | ppl 1.000\n",
      "35998it [18:41, 45.42it/s]Train   1: iter 36000 | loss 0.000 | ppl 1.000\n",
      "36996it [19:07, 41.78it/s]Train   1: iter 37000 | loss 0.000 | ppl 1.000\n",
      "38000it [19:34, 31.68it/s]Train   1: iter 38000 | loss 0.000 | ppl 1.000\n",
      "39000it [20:01, 36.40it/s]Train   1: iter 39000 | loss 0.000 | ppl 1.000\n",
      "39999it [20:32, 29.89it/s]Train   1: iter 40000 | loss 0.000 | ppl 1.000\n",
      "Val   1: iter 40000 | loss 0.000 | ppl 1.000\n",
      "40997it [21:11, 36.35it/s]Train   1: iter 41000 | loss 0.000 | ppl 1.000\n",
      "41999it [21:40, 34.80it/s]Train   1: iter 42000 | loss 0.000 | ppl 1.000\n",
      "42999it [22:10, 32.72it/s]Train   1: iter 43000 | loss 0.000 | ppl 1.000\n",
      "44000it [22:37, 42.93it/s]Train   1: iter 44000 | loss 0.000 | ppl 1.000\n",
      "44997it [23:02, 42.79it/s]Train   1: iter 45000 | loss 0.000 | ppl 1.000\n",
      "45999it [23:26, 41.05it/s]Train   1: iter 46000 | loss 0.000 | ppl 1.000\n",
      "46998it [23:51, 39.63it/s]Train   1: iter 47000 | loss 0.000 | ppl 1.000\n",
      "47999it [24:16, 42.61it/s]Train   1: iter 48000 | loss 0.000 | ppl 1.000\n",
      "48997it [24:39, 38.53it/s]Train   1: iter 49000 | loss 0.000 | ppl 1.000\n",
      "50000it [25:04, 42.96it/s]Train   1: iter 50000 | loss 0.000 | ppl 1.000\n",
      "Val   1: iter 50000 | loss 0.000 | ppl 1.000\n",
      "[!] saving model...\n",
      "51000it [25:37, 39.52it/s]Train   1: iter 51000 | loss 0.000 | ppl 1.000\n",
      "52000it [26:03, 39.96it/s]Train   1: iter 52000 | loss 0.000 | ppl 1.000\n",
      "52996it [26:28, 43.38it/s]Train   1: iter 53000 | loss 0.000 | ppl 1.000\n",
      "53998it [26:54, 30.73it/s]Train   1: iter 54000 | loss 0.000 | ppl 1.000\n",
      "55000it [27:19, 37.95it/s]Train   1: iter 55000 | loss 0.000 | ppl 1.000\n",
      "55997it [27:44, 42.67it/s]Train   1: iter 56000 | loss 0.000 | ppl 1.000\n",
      "56997it [28:07, 37.16it/s]Train   1: iter 57000 | loss 0.000 | ppl 1.000\n",
      "58000it [28:32, 42.33it/s]Train   1: iter 58000 | loss 0.000 | ppl 1.000\n",
      "58997it [28:56, 42.76it/s]Train   1: iter 59000 | loss 0.000 | ppl 1.000\n",
      "59997it [29:23, 39.55it/s]Train   1: iter 60000 | loss 0.000 | ppl 1.000\n",
      "Val   1: iter 60000 | loss 0.000 | ppl 1.000\n",
      "60998it [30:03, 29.50it/s]Train   1: iter 61000 | loss 0.000 | ppl 1.000\n",
      "62000it [30:33, 34.52it/s]Train   1: iter 62000 | loss 0.000 | ppl 1.000\n",
      "62999it [31:03, 39.78it/s]Train   1: iter 63000 | loss 0.000 | ppl 1.000\n",
      "63997it [31:33, 32.59it/s]Train   1: iter 64000 | loss 0.000 | ppl 1.000\n",
      "64996it [31:59, 43.20it/s]Train   1: iter 65000 | loss 0.000 | ppl 1.000\n",
      "65998it [32:24, 40.40it/s]Train   1: iter 66000 | loss 0.000 | ppl 1.000\n",
      "66999it [32:48, 42.83it/s]Train   1: iter 67000 | loss 0.000 | ppl 1.000\n",
      "67998it [33:12, 43.10it/s]Train   1: iter 68000 | loss 0.000 | ppl 1.000\n",
      "69000it [33:36, 39.90it/s]Train   1: iter 69000 | loss 0.000 | ppl 1.000\n",
      "69996it [34:00, 45.60it/s]Train   1: iter 70000 | loss 0.000 | ppl 1.000\n",
      "Val   1: iter 70000 | loss 0.000 | ppl 1.000\n",
      "[!] saving model...\n",
      "70996it [34:36, 41.56it/s]Train   1: iter 71000 | loss 0.000 | ppl 1.000\n",
      "71999it [35:05, 33.32it/s]Train   1: iter 72000 | loss 0.000 | ppl 1.000\n",
      "72998it [35:33, 30.29it/s]Train   1: iter 73000 | loss 0.000 | ppl 1.000\n",
      "73997it [36:03, 40.34it/s]Train   1: iter 74000 | loss 0.000 | ppl 1.000\n",
      "74998it [36:28, 39.69it/s]Train   1: iter 75000 | loss 0.000 | ppl 1.000\n",
      "75999it [36:54, 41.79it/s]Train   1: iter 76000 | loss 0.000 | ppl 1.000\n",
      "77000it [37:19, 42.29it/s]Train   1: iter 77000 | loss 0.000 | ppl 1.000\n",
      "78000it [37:43, 42.38it/s]Train   1: iter 78000 | loss 0.000 | ppl 1.000\n",
      "78999it [38:08, 42.94it/s]Train   1: iter 79000 | loss 0.000 | ppl 1.000\n",
      "79997it [38:34, 38.70it/s]Train   1: iter 80000 | loss 0.000 | ppl 1.000\n",
      "Val   1: iter 80000 | loss 0.000 | ppl 1.000\n",
      "80999it [39:13, 29.00it/s]Train   1: iter 81000 | loss 0.000 | ppl 1.000\n",
      "82000it [39:43, 35.62it/s]Train   1: iter 82000 | loss 0.000 | ppl 1.000\n",
      "82999it [40:12, 29.64it/s]Train   1: iter 83000 | loss 0.000 | ppl 1.000\n",
      "83997it [40:43, 39.70it/s]Train   1: iter 84000 | loss 0.000 | ppl 1.000\n",
      "84999it [41:08, 42.28it/s]Train   1: iter 85000 | loss 0.000 | ppl 1.000\n",
      "85996it [41:34, 42.50it/s]Train   1: iter 86000 | loss 0.000 | ppl 1.000\n",
      "86997it [41:59, 42.54it/s]Train   1: iter 87000 | loss 0.000 | ppl 1.000\n",
      "87999it [42:23, 31.35it/s]Train   1: iter 88000 | loss 0.000 | ppl 1.000\n",
      "88996it [42:49, 44.01it/s]Train   1: iter 89000 | loss 0.000 | ppl 1.000\n",
      "90000it [43:17, 35.35it/s]Train   1: iter 90000 | loss 0.000 | ppl 1.000\n",
      "Val   1: iter 90000 | loss 0.000 | ppl 1.000\n",
      "90999it [43:55, 39.64it/s]Train   1: iter 91000 | loss 0.000 | ppl 1.000\n",
      "92000it [44:23, 32.20it/s]Train   1: iter 92000 | loss 0.000 | ppl 1.000\n",
      "93000it [44:48, 39.29it/s]Train   1: iter 93000 | loss 0.000 | ppl 1.000\n",
      "93997it [45:14, 41.69it/s]Train   1: iter 94000 | loss 0.000 | ppl 1.000\n",
      "94997it [45:38, 42.44it/s]Train   1: iter 95000 | loss 0.000 | ppl 1.000\n",
      "95997it [46:03, 32.84it/s]Train   1: iter 96000 | loss 0.000 | ppl 1.000\n",
      "96997it [46:29, 37.98it/s]Train   1: iter 97000 | loss 0.000 | ppl 1.000\n",
      "97999it [46:54, 34.82it/s]Train   1: iter 98000 | loss 0.000 | ppl 1.000\n",
      "98997it [47:21, 31.81it/s]Train   1: iter 99000 | loss 0.000 | ppl 1.000\n",
      "100000it [47:49, 39.44it/s]Train   1: iter 100000 | loss 0.000 | ppl 1.000\n",
      "Val   1: iter 100000 | loss 0.000 | ppl 1.000\n",
      "100999it [48:28, 29.97it/s]Train   1: iter 101000 | loss 0.000 | ppl 1.000\n",
      "101997it [48:57, 40.62it/s]Train   1: iter 102000 | loss 0.000 | ppl 1.000\n",
      "102997it [49:26, 41.13it/s]Train   1: iter 103000 | loss 0.000 | ppl 1.000\n",
      "104000it [49:55, 38.48it/s]Train   1: iter 104000 | loss 0.000 | ppl 1.000\n",
      "104997it [50:25, 35.15it/s]Train   1: iter 105000 | loss 0.000 | ppl 1.000\n",
      "105996it [50:54, 32.94it/s]Train   1: iter 106000 | loss 0.000 | ppl 1.000\n",
      "107000it [51:26, 30.25it/s]Train   1: iter 107000 | loss 0.000 | ppl 1.000\n",
      "107998it [51:57, 38.17it/s]Train   1: iter 108000 | loss 0.000 | ppl 1.000\n",
      "109000it [52:29, 38.85it/s]Train   1: iter 109000 | loss 0.000 | ppl 1.000\n",
      "109996it [53:00, 33.39it/s]Train   1: iter 110000 | loss 0.000 | ppl 1.000\n",
      "Val   1: iter 110000 | loss 0.000 | ppl 1.000\n",
      "110999it [53:40, 31.13it/s]Train   1: iter 111000 | loss 0.000 | ppl 1.000\n",
      "111996it [54:11, 43.04it/s]Train   1: iter 112000 | loss 0.000 | ppl 1.000\n",
      "112997it [54:41, 35.71it/s]Train   1: iter 113000 | loss 0.000 | ppl 1.000\n",
      "114000it [55:10, 41.56it/s]Train   1: iter 114000 | loss 0.000 | ppl 1.000\n",
      "115000it [55:40, 35.28it/s]Train   1: iter 115000 | loss 0.000 | ppl 1.000\n",
      "115998it [56:11, 29.48it/s]Train   1: iter 116000 | loss 0.000 | ppl 1.000\n",
      "116996it [56:41, 39.91it/s]Train   1: iter 117000 | loss 0.000 | ppl 1.000\n",
      "117999it [57:11, 34.77it/s]Train   1: iter 118000 | loss 0.000 | ppl 1.000\n",
      "119000it [57:41, 34.18it/s]Train   1: iter 119000 | loss 0.000 | ppl 1.000\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "119999it [58:12, 30.80it/s]Train   1: iter 120000 | loss 0.000 | ppl 1.000\n",
      "Val   1: iter 120000 | loss 0.000 | ppl 1.000\n",
      "121000it [58:50, 29.57it/s]Train   1: iter 121000 | loss 0.000 | ppl 1.000\n",
      "121999it [59:18, 32.01it/s]Train   1: iter 122000 | loss 0.000 | ppl 1.000\n",
      "122996it [59:46, 40.84it/s]Train   1: iter 123000 | loss 0.000 | ppl 1.000\n",
      "123417it [59:57, 34.30it/s]\n",
      "0it [00:00, ?it/s]Train   2: iter     0 | loss 0.000 | ppl 1.000\n",
      "Val   2: iter     0 | loss 0.000 | ppl 1.000\n",
      "998it [00:39, 33.98it/s]Train   2: iter  1000 | loss 0.000 | ppl 1.000\n",
      "1998it [01:11, 35.17it/s]Train   2: iter  2000 | loss 0.000 | ppl 1.000\n",
      "2999it [01:41, 30.56it/s]Train   2: iter  3000 | loss 0.000 | ppl 1.000\n",
      "3998it [02:06, 39.12it/s]Train   2: iter  4000 | loss 0.000 | ppl 1.000\n",
      "4996it [02:32, 43.53it/s]Train   2: iter  5000 | loss 0.000 | ppl 1.000\n",
      "5999it [02:57, 38.89it/s]Train   2: iter  6000 | loss 0.000 | ppl 1.000\n",
      "6998it [03:23, 41.39it/s]Train   2: iter  7000 | loss 0.000 | ppl 1.000\n",
      "7997it [03:48, 40.29it/s]Train   2: iter  8000 | loss 0.000 | ppl 1.000\n",
      "9000it [04:13, 39.40it/s]Train   2: iter  9000 | loss 0.000 | ppl 1.000\n",
      "9998it [04:38, 39.56it/s]Train   2: iter 10000 | loss 0.000 | ppl 1.000\n",
      "Val   2: iter 10000 | loss 0.000 | ppl 1.000\n",
      "[!] saving model...\n",
      "10998it [05:18, 32.73it/s]Train   2: iter 11000 | loss 0.000 | ppl 1.000\n",
      "11999it [05:48, 35.52it/s]Train   2: iter 12000 | loss 0.000 | ppl 1.000\n",
      "13000it [06:17, 39.18it/s]Train   2: iter 13000 | loss 0.000 | ppl 1.000\n",
      "14000it [06:47, 27.07it/s]Train   2: iter 14000 | loss 0.000 | ppl 1.000\n",
      "14998it [07:17, 32.18it/s]Train   2: iter 15000 | loss 0.000 | ppl 1.000\n",
      "16000it [07:49, 25.76it/s]Train   2: iter 16000 | loss 0.000 | ppl 1.000\n",
      "17000it [08:22, 33.81it/s]Train   2: iter 17000 | loss 0.000 | ppl 1.000\n",
      "17999it [08:53, 30.51it/s]Train   2: iter 18000 | loss 0.000 | ppl 1.000\n",
      "18999it [09:25, 38.62it/s]Train   2: iter 19000 | loss 0.000 | ppl 1.000\n",
      "20000it [09:54, 31.29it/s]Train   2: iter 20000 | loss 0.000 | ppl 1.000\n",
      "Val   2: iter 20000 | loss 0.000 | ppl 1.000\n",
      "[!] saving model...\n",
      "21000it [10:37, 26.43it/s]Train   2: iter 21000 | loss 0.000 | ppl 1.000\n",
      "21999it [11:11, 25.82it/s]Train   2: iter 22000 | loss 0.000 | ppl 1.000\n",
      "22997it [11:42, 36.92it/s]Train   2: iter 23000 | loss 0.000 | ppl 1.000\n",
      "23998it [12:10, 34.31it/s]Train   2: iter 24000 | loss 0.000 | ppl 1.000\n",
      "24998it [12:39, 33.77it/s]Train   2: iter 25000 | loss 0.000 | ppl 1.000\n",
      "25998it [13:07, 39.12it/s]Train   2: iter 26000 | loss 0.000 | ppl 1.000\n",
      "26999it [13:35, 39.04it/s]Train   2: iter 27000 | loss 0.000 | ppl 1.000\n",
      "27999it [14:01, 39.42it/s]Train   2: iter 28000 | loss 0.000 | ppl 1.000\n",
      "29000it [14:28, 25.66it/s]Train   2: iter 29000 | loss 0.000 | ppl 1.000\n",
      "29999it [14:59, 33.22it/s]Train   2: iter 30000 | loss 0.000 | ppl 1.000\n",
      "Val   2: iter 30000 | loss 0.000 | ppl 1.000\n",
      "31000it [15:38, 26.99it/s]Train   2: iter 31000 | loss 0.000 | ppl 1.000\n",
      "32000it [16:08, 32.86it/s]Train   2: iter 32000 | loss 0.000 | ppl 1.000\n",
      "32997it [16:37, 41.91it/s]Train   2: iter 33000 | loss 0.000 | ppl 1.000\n",
      "33998it [17:02, 40.17it/s]Train   2: iter 34000 | loss 0.000 | ppl 1.000\n",
      "34998it [17:27, 42.37it/s]Train   2: iter 35000 | loss 0.000 | ppl 1.000\n",
      "36000it [17:52, 43.28it/s]Train   2: iter 36000 | loss 0.000 | ppl 1.000\n",
      "37000it [18:18, 33.96it/s]Train   2: iter 37000 | loss 0.000 | ppl 1.000\n",
      "37997it [18:45, 39.28it/s]Train   2: iter 38000 | loss 0.000 | ppl 1.000\n",
      "39000it [19:15, 35.77it/s]Train   2: iter 39000 | loss 0.000 | ppl 1.000\n",
      "39999it [19:45, 36.03it/s]Train   2: iter 40000 | loss 0.000 | ppl 1.000\n",
      "Val   2: iter 40000 | loss 0.000 | ppl 1.000\n",
      "40999it [20:25, 31.67it/s]Train   2: iter 41000 | loss 0.000 | ppl 1.000\n",
      "41997it [20:54, 26.91it/s]Train   2: iter 42000 | loss 0.000 | ppl 1.000\n",
      "42997it [21:23, 33.37it/s]Train   2: iter 43000 | loss 0.000 | ppl 1.000\n",
      "44000it [21:50, 33.13it/s]Train   2: iter 44000 | loss 0.000 | ppl 1.000\n",
      "44999it [22:21, 33.37it/s]Train   2: iter 45000 | loss 0.000 | ppl 1.000\n",
      "45997it [22:50, 32.26it/s]Train   2: iter 46000 | loss 0.000 | ppl 1.000\n",
      "46997it [23:21, 35.83it/s]Train   2: iter 47000 | loss 0.000 | ppl 1.000\n",
      "47999it [23:54, 32.27it/s]Train   2: iter 48000 | loss 0.000 | ppl 1.000\n",
      "48999it [24:25, 29.52it/s]Train   2: iter 49000 | loss 0.000 | ppl 1.000\n",
      "49999it [24:55, 29.85it/s]Train   2: iter 50000 | loss 0.000 | ppl 1.000\n",
      "Val   2: iter 50000 | loss 0.000 | ppl 1.000\n",
      "50998it [25:37, 27.33it/s]Train   2: iter 51000 | loss 0.000 | ppl 1.000\n",
      "52000it [26:08, 40.28it/s]Train   2: iter 52000 | loss 0.000 | ppl 1.000\n",
      "52997it [26:34, 39.82it/s]Train   2: iter 53000 | loss 0.000 | ppl 1.000\n",
      "53999it [27:04, 37.46it/s]Train   2: iter 54000 | loss 0.000 | ppl 1.000\n",
      "54998it [27:35, 30.26it/s]Train   2: iter 55000 | loss 0.000 | ppl 1.000\n",
      "56000it [28:07, 32.85it/s]Train   2: iter 56000 | loss 0.000 | ppl 1.000\n",
      "56997it [28:37, 36.92it/s]Train   2: iter 57000 | loss 0.000 | ppl 1.000\n",
      "58000it [29:09, 35.73it/s]Train   2: iter 58000 | loss 0.000 | ppl 1.000\n",
      "58999it [29:42, 26.14it/s]Train   2: iter 59000 | loss 0.000 | ppl 1.000\n",
      "59997it [30:14, 35.65it/s]Train   2: iter 60000 | loss 0.000 | ppl 1.000\n",
      "Val   2: iter 60000 | loss 0.000 | ppl 1.000\n",
      "60997it [30:52, 31.44it/s]Train   2: iter 61000 | loss 0.000 | ppl 1.000\n",
      "61996it [31:21, 39.37it/s]Train   2: iter 62000 | loss 0.000 | ppl 1.000\n",
      "62996it [31:47, 41.88it/s]Train   2: iter 63000 | loss 0.000 | ppl 1.000\n",
      "63997it [32:12, 38.66it/s]Train   2: iter 64000 | loss 0.000 | ppl 1.000\n",
      "65000it [32:36, 42.88it/s]Train   2: iter 65000 | loss 0.000 | ppl 1.000\n",
      "65996it [33:01, 42.62it/s]Train   2: iter 66000 | loss 0.000 | ppl 1.000\n",
      "67000it [33:27, 36.50it/s]Train   2: iter 67000 | loss 0.000 | ppl 1.000\n",
      "68000it [33:53, 38.17it/s]Train   2: iter 68000 | loss 0.000 | ppl 1.000\n",
      "68998it [34:22, 39.01it/s]Train   2: iter 69000 | loss 0.000 | ppl 1.000\n",
      "69997it [34:49, 38.60it/s]Train   2: iter 70000 | loss 0.000 | ppl 1.000\n",
      "Val   2: iter 70000 | loss 0.000 | ppl 1.000\n",
      "[!] saving model...\n",
      "70999it [35:25, 32.10it/s]Train   2: iter 71000 | loss 0.000 | ppl 1.000\n",
      "71997it [35:53, 31.79it/s]Train   2: iter 72000 | loss 0.000 | ppl 1.000\n",
      "72997it [36:21, 39.58it/s]Train   2: iter 73000 | loss 0.000 | ppl 1.000\n",
      "73999it [36:47, 39.62it/s]Train   2: iter 74000 | loss 0.000 | ppl 1.000\n",
      "74998it [37:13, 39.21it/s]Train   2: iter 75000 | loss 0.000 | ppl 1.000\n",
      "75999it [37:39, 39.09it/s]Train   2: iter 76000 | loss 0.000 | ppl 1.000\n",
      "77000it [38:06, 34.69it/s]Train   2: iter 77000 | loss 0.000 | ppl 1.000\n",
      "78000it [38:32, 39.56it/s]Train   2: iter 78000 | loss 0.000 | ppl 1.000\n",
      "78997it [38:56, 41.37it/s]Train   2: iter 79000 | loss 0.000 | ppl 1.000\n",
      "79998it [39:21, 42.09it/s]Train   2: iter 80000 | loss 0.000 | ppl 1.000\n",
      "Val   2: iter 80000 | loss 0.000 | ppl 1.000\n",
      "80997it [39:56, 31.63it/s]Train   2: iter 81000 | loss 0.000 | ppl 1.000\n",
      "81998it [40:24, 36.65it/s]Train   2: iter 82000 | loss 0.000 | ppl 1.000\n",
      "83000it [40:55, 31.19it/s]Train   2: iter 83000 | loss 0.000 | ppl 1.000\n",
      "84000it [41:25, 37.23it/s]Train   2: iter 84000 | loss 0.000 | ppl 1.000\n",
      "84997it [41:55, 36.11it/s]Train   2: iter 85000 | loss 0.000 | ppl 1.000\n",
      "86000it [42:28, 34.01it/s]Train   2: iter 86000 | loss 0.000 | ppl 1.000\n",
      "86998it [43:02, 29.54it/s]Train   2: iter 87000 | loss 0.000 | ppl 1.000\n",
      "87998it [43:35, 26.07it/s]Train   2: iter 88000 | loss 0.000 | ppl 1.000\n",
      "89000it [44:08, 27.39it/s]Train   2: iter 89000 | loss 0.000 | ppl 1.000\n",
      "90000it [44:44, 26.61it/s]Train   2: iter 90000 | loss 0.000 | ppl 1.000\n",
      "Val   2: iter 90000 | loss 0.000 | ppl 1.000\n",
      "90997it [45:26, 31.87it/s]Train   2: iter 91000 | loss 0.000 | ppl 1.000\n",
      "92000it [45:56, 36.37it/s]Train   2: iter 92000 | loss 0.000 | ppl 1.000\n",
      "93000it [46:26, 37.86it/s]Train   2: iter 93000 | loss 0.000 | ppl 1.000\n",
      "93999it [46:54, 29.37it/s]Train   2: iter 94000 | loss 0.000 | ppl 1.000\n",
      "94998it [47:25, 32.05it/s]Train   2: iter 95000 | loss 0.000 | ppl 1.000\n",
      "96000it [47:56, 26.30it/s]Train   2: iter 96000 | loss 0.000 | ppl 1.000\n",
      "97000it [48:28, 28.40it/s]Train   2: iter 97000 | loss 0.000 | ppl 1.000\n",
      "97998it [49:01, 32.09it/s]Train   2: iter 98000 | loss 0.000 | ppl 1.000\n",
      "98999it [49:35, 32.72it/s]Train   2: iter 99000 | loss 0.000 | ppl 1.000\n",
      "100000it [50:06, 39.53it/s]Train   2: iter 100000 | loss 0.000 | ppl 1.000\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Val   2: iter 100000 | loss 0.000 | ppl 1.000\n",
      "100998it [50:45, 35.45it/s]Train   2: iter 101000 | loss 0.000 | ppl 1.000\n",
      "101999it [51:13, 39.43it/s]Train   2: iter 102000 | loss 0.000 | ppl 1.000\n",
      "103000it [51:43, 33.28it/s]Train   2: iter 103000 | loss 0.000 | ppl 1.000\n",
      "103996it [52:16, 34.21it/s]Train   2: iter 104000 | loss 0.000 | ppl 1.000\n",
      "104997it [52:45, 37.32it/s]Train   2: iter 105000 | loss 0.000 | ppl 1.000\n",
      "105999it [53:14, 39.88it/s]Train   2: iter 106000 | loss 0.000 | ppl 1.000\n",
      "107000it [53:42, 32.77it/s]Train   2: iter 107000 | loss 0.000 | ppl 1.000\n",
      "108000it [54:11, 33.23it/s]Train   2: iter 108000 | loss 0.000 | ppl 1.000\n",
      "109000it [54:42, 35.31it/s]Train   2: iter 109000 | loss 0.000 | ppl 1.000\n",
      "109998it [55:13, 26.68it/s]Train   2: iter 110000 | loss 0.000 | ppl 1.000\n",
      "Val   2: iter 110000 | loss 0.000 | ppl 1.000\n",
      "110999it [55:55, 32.14it/s]Train   2: iter 111000 | loss 0.000 | ppl 1.000\n",
      "112000it [56:26, 33.92it/s]Train   2: iter 112000 | loss 0.000 | ppl 1.000\n",
      "112998it [56:54, 30.96it/s]Train   2: iter 113000 | loss 0.000 | ppl 1.000\n",
      "114000it [57:20, 38.22it/s]Train   2: iter 114000 | loss 0.000 | ppl 1.000\n",
      "114997it [57:46, 39.12it/s]Train   2: iter 115000 | loss 0.000 | ppl 1.000\n",
      "115999it [58:11, 35.79it/s]Train   2: iter 116000 | loss 0.000 | ppl 1.000\n",
      "117000it [58:37, 39.64it/s]Train   2: iter 117000 | loss 0.000 | ppl 1.000\n",
      "117996it [59:03, 36.61it/s]Train   2: iter 118000 | loss 0.000 | ppl 1.000\n",
      "118999it [59:29, 32.29it/s]Train   2: iter 119000 | loss 0.000 | ppl 1.000\n",
      "119999it [59:55, 42.78it/s]Train   2: iter 120000 | loss 0.000 | ppl 1.000\n",
      "Val   2: iter 120000 | loss 0.000 | ppl 1.000\n",
      "120998it [1:00:36, 32.59it/s]Train   2: iter 121000 | loss 0.000 | ppl 1.000\n",
      "121997it [1:01:09, 37.94it/s]Train   2: iter 122000 | loss 0.000 | ppl 1.000\n",
      "122997it [1:01:43, 27.85it/s]Train   2: iter 123000 | loss 0.000 | ppl 1.000\n",
      "123417it [1:01:57, 33.20it/s]\n",
      "0it [00:00, ?it/s]Train   3: iter     0 | loss 0.000 | ppl 1.000\n",
      "Val   3: iter     0 | loss 0.000 | ppl 1.000\n",
      "999it [00:35, 39.56it/s]Train   3: iter  1000 | loss 0.000 | ppl 1.000\n",
      "1999it [01:02, 37.94it/s]Train   3: iter  2000 | loss 0.000 | ppl 1.000\n",
      "2997it [01:27, 41.96it/s]Train   3: iter  3000 | loss 0.000 | ppl 1.000\n",
      "4000it [01:56, 35.09it/s]Train   3: iter  4000 | loss 0.000 | ppl 1.000\n",
      "4997it [02:23, 39.54it/s]Train   3: iter  5000 | loss 0.000 | ppl 1.000\n",
      "5996it [02:49, 40.12it/s]Train   3: iter  6000 | loss 0.000 | ppl 1.000\n",
      "6998it [03:17, 30.09it/s]Train   3: iter  7000 | loss 0.000 | ppl 1.000\n",
      "8000it [03:48, 33.14it/s]Train   3: iter  8000 | loss 0.000 | ppl 1.000\n",
      "8997it [04:17, 26.88it/s]Train   3: iter  9000 | loss 0.000 | ppl 1.000\n",
      "9998it [04:49, 31.20it/s]Train   3: iter 10000 | loss 0.000 | ppl 1.000\n",
      "Val   3: iter 10000 | loss 0.000 | ppl 1.000\n",
      "10998it [05:24, 38.65it/s]Train   3: iter 11000 | loss 0.000 | ppl 1.000\n",
      "11998it [05:52, 39.14it/s]Train   3: iter 12000 | loss 0.000 | ppl 1.000\n",
      "12998it [06:19, 41.34it/s]Train   3: iter 13000 | loss 0.000 | ppl 1.000\n",
      "14000it [06:45, 36.94it/s]Train   3: iter 14000 | loss 0.000 | ppl 1.000\n",
      "14997it [07:13, 40.34it/s]Train   3: iter 15000 | loss 0.000 | ppl 1.000\n",
      "15996it [07:37, 39.91it/s]Train   3: iter 16000 | loss 0.000 | ppl 1.000\n",
      "16997it [08:02, 42.67it/s]Train   3: iter 17000 | loss 0.000 | ppl 1.000\n",
      "18000it [08:27, 42.68it/s]Train   3: iter 18000 | loss 0.000 | ppl 1.000\n",
      "18997it [08:51, 42.96it/s]Train   3: iter 19000 | loss 0.000 | ppl 1.000\n",
      "20000it [09:15, 42.99it/s]Train   3: iter 20000 | loss 0.000 | ppl 1.000\n",
      "Val   3: iter 20000 | loss 0.000 | ppl 1.000\n",
      "21000it [09:48, 41.84it/s]Train   3: iter 21000 | loss 0.000 | ppl 1.000\n",
      "22000it [10:14, 39.71it/s]Train   3: iter 22000 | loss 0.000 | ppl 1.000\n",
      "22998it [10:38, 42.96it/s]Train   3: iter 23000 | loss 0.000 | ppl 1.000\n",
      "23998it [11:01, 43.03it/s]Train   3: iter 24000 | loss 0.000 | ppl 1.000\n",
      "24998it [11:26, 39.55it/s]Train   3: iter 25000 | loss 0.000 | ppl 1.000\n",
      "25996it [11:51, 43.35it/s]Train   3: iter 26000 | loss 0.000 | ppl 1.000\n",
      "26998it [12:16, 36.77it/s]Train   3: iter 27000 | loss 0.000 | ppl 1.000\n",
      "28000it [12:42, 39.76it/s]Train   3: iter 28000 | loss 0.000 | ppl 1.000\n",
      "29000it [13:07, 43.15it/s]Train   3: iter 29000 | loss 0.000 | ppl 1.000\n",
      "29998it [13:33, 39.10it/s]Train   3: iter 30000 | loss 0.000 | ppl 1.000\n",
      "Val   3: iter 30000 | loss 0.000 | ppl 1.000\n",
      "30998it [14:08, 34.83it/s]Train   3: iter 31000 | loss 0.000 | ppl 1.000\n",
      "31999it [14:32, 39.93it/s]Train   3: iter 32000 | loss 0.000 | ppl 1.000\n",
      "32999it [14:58, 40.08it/s]Train   3: iter 33000 | loss 0.000 | ppl 1.000\n",
      "34000it [15:24, 39.71it/s]Train   3: iter 34000 | loss 0.000 | ppl 1.000\n",
      "34998it [15:51, 39.62it/s]Train   3: iter 35000 | loss 0.000 | ppl 1.000\n",
      "36000it [16:16, 40.02it/s]Train   3: iter 36000 | loss 0.000 | ppl 1.000\n",
      "36997it [16:41, 36.42it/s]Train   3: iter 37000 | loss 0.000 | ppl 1.000\n",
      "37998it [17:07, 37.07it/s]Train   3: iter 38000 | loss 0.000 | ppl 1.000\n",
      "38999it [17:32, 39.14it/s]Train   3: iter 39000 | loss 0.000 | ppl 1.000\n",
      "39998it [17:57, 35.43it/s]Train   3: iter 40000 | loss 0.000 | ppl 1.000\n",
      "Val   3: iter 40000 | loss 0.000 | ppl 1.000\n",
      "41000it [18:31, 38.85it/s]Train   3: iter 41000 | loss 0.000 | ppl 1.000\n",
      "42000it [18:57, 39.58it/s]Train   3: iter 42000 | loss 0.000 | ppl 1.000\n",
      "42996it [19:24, 39.83it/s]Train   3: iter 43000 | loss 0.000 | ppl 1.000\n",
      "43998it [19:50, 38.40it/s]Train   3: iter 44000 | loss 0.000 | ppl 1.000\n",
      "44998it [20:15, 40.13it/s]Train   3: iter 45000 | loss 0.000 | ppl 1.000\n",
      "46000it [20:40, 43.13it/s]Train   3: iter 46000 | loss 0.000 | ppl 1.000\n",
      "46997it [21:04, 42.31it/s]Train   3: iter 47000 | loss 0.000 | ppl 1.000\n",
      "47999it [21:29, 39.38it/s]Train   3: iter 48000 | loss 0.000 | ppl 1.000\n",
      "48997it [21:53, 38.51it/s]Train   3: iter 49000 | loss 0.000 | ppl 1.000\n",
      "49998it [22:19, 38.63it/s]Train   3: iter 50000 | loss 0.000 | ppl 1.000\n",
      "Val   3: iter 50000 | loss 0.000 | ppl 1.000\n",
      "50997it [22:56, 33.81it/s]Train   3: iter 51000 | loss 0.000 | ppl 1.000\n",
      "52000it [23:22, 39.61it/s]Train   3: iter 52000 | loss 0.000 | ppl 1.000\n",
      "52998it [23:47, 41.80it/s]Train   3: iter 53000 | loss 0.000 | ppl 1.000\n",
      "53997it [24:12, 42.70it/s]Train   3: iter 54000 | loss 0.000 | ppl 1.000\n",
      "55000it [24:36, 32.43it/s]Train   3: iter 55000 | loss 0.000 | ppl 1.000\n",
      "55996it [25:00, 42.73it/s]Train   3: iter 56000 | loss 0.000 | ppl 1.000\n",
      "56999it [25:25, 42.87it/s]Train   3: iter 57000 | loss 0.000 | ppl 1.000\n",
      "57996it [25:49, 39.65it/s]Train   3: iter 58000 | loss 0.000 | ppl 1.000\n",
      "58998it [26:15, 39.44it/s]Train   3: iter 59000 | loss 0.000 | ppl 1.000\n",
      "59999it [26:40, 39.63it/s]Train   3: iter 60000 | loss 0.000 | ppl 1.000\n",
      "Val   3: iter 60000 | loss 0.000 | ppl 1.000\n",
      "60996it [27:14, 45.65it/s]Train   3: iter 61000 | loss 0.000 | ppl 1.000\n",
      "61997it [27:38, 43.16it/s]Train   3: iter 62000 | loss 0.000 | ppl 1.000\n",
      "63000it [28:02, 39.60it/s]Train   3: iter 63000 | loss 0.000 | ppl 1.000\n",
      "63998it [28:28, 38.83it/s]Train   3: iter 64000 | loss 0.000 | ppl 1.000\n",
      "65000it [28:53, 35.51it/s]Train   3: iter 65000 | loss 0.000 | ppl 1.000\n",
      "66000it [29:19, 36.17it/s]Train   3: iter 66000 | loss 0.000 | ppl 1.000\n",
      "66998it [29:43, 40.20it/s]Train   3: iter 67000 | loss 0.000 | ppl 1.000\n",
      "68000it [30:08, 40.22it/s]Train   3: iter 68000 | loss 0.000 | ppl 1.000\n",
      "69000it [30:33, 39.60it/s]Train   3: iter 69000 | loss 0.000 | ppl 1.000\n",
      "69998it [30:59, 38.66it/s]Train   3: iter 70000 | loss 0.000 | ppl 1.000\n",
      "Val   3: iter 70000 | loss 0.000 | ppl 1.000\n",
      "71000it [31:33, 42.12it/s]Train   3: iter 71000 | loss 0.000 | ppl 1.000\n",
      "71997it [31:57, 39.68it/s]Train   3: iter 72000 | loss 0.000 | ppl 1.000\n",
      "72998it [32:21, 45.81it/s]Train   3: iter 73000 | loss 0.000 | ppl 1.000\n",
      "73999it [32:45, 39.53it/s]Train   3: iter 74000 | loss 0.000 | ppl 1.000\n",
      "74996it [33:12, 40.25it/s]Train   3: iter 75000 | loss 0.000 | ppl 1.000\n",
      "75997it [33:40, 35.34it/s]Train   3: iter 76000 | loss 0.000 | ppl 1.000\n",
      "76999it [34:10, 27.09it/s]Train   3: iter 77000 | loss 0.000 | ppl 1.000\n",
      "77997it [34:42, 33.53it/s]Train   3: iter 78000 | loss 0.000 | ppl 1.000\n",
      "79000it [35:13, 30.20it/s]Train   3: iter 79000 | loss 0.000 | ppl 1.000\n",
      "79997it [35:44, 29.61it/s]Train   3: iter 80000 | loss 0.000 | ppl 1.000\n",
      "Val   3: iter 80000 | loss 0.000 | ppl 1.000\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "80999it [36:24, 35.12it/s]Train   3: iter 81000 | loss 0.000 | ppl 1.000\n",
      "82000it [36:54, 35.32it/s]Train   3: iter 82000 | loss 0.000 | ppl 1.000\n",
      "82999it [37:25, 32.71it/s]Train   3: iter 83000 | loss 0.000 | ppl 1.000\n",
      "83997it [37:56, 40.54it/s]Train   3: iter 84000 | loss 0.000 | ppl 1.000\n",
      "84996it [38:23, 33.00it/s]Train   3: iter 85000 | loss 0.000 | ppl 1.000\n",
      "85998it [38:52, 37.48it/s]Train   3: iter 86000 | loss 0.000 | ppl 1.000\n",
      "86998it [39:22, 31.42it/s]Train   3: iter 87000 | loss 0.000 | ppl 1.000\n",
      "87999it [39:54, 36.53it/s]Train   3: iter 88000 | loss 0.000 | ppl 1.000\n",
      "88999it [40:26, 30.33it/s]Train   3: iter 89000 | loss 0.000 | ppl 1.000\n",
      "89997it [40:59, 31.52it/s]Train   3: iter 90000 | loss 0.000 | ppl 1.000\n",
      "Val   3: iter 90000 | loss 0.000 | ppl 1.000\n",
      "91000it [41:39, 32.31it/s]Train   3: iter 91000 | loss 0.000 | ppl 1.000\n",
      "91997it [42:12, 33.31it/s]Train   3: iter 92000 | loss 0.000 | ppl 1.000\n",
      "92998it [42:42, 32.38it/s]Train   3: iter 93000 | loss 0.000 | ppl 1.000\n",
      "93999it [43:14, 30.35it/s]Train   3: iter 94000 | loss 0.000 | ppl 1.000\n",
      "94997it [43:46, 29.45it/s]Train   3: iter 95000 | loss 0.000 | ppl 1.000\n",
      "96000it [44:20, 27.89it/s]Train   3: iter 96000 | loss 0.000 | ppl 1.000\n",
      "96999it [44:51, 32.66it/s]Train   3: iter 97000 | loss 0.000 | ppl 1.000\n",
      "97998it [45:20, 38.42it/s]Train   3: iter 98000 | loss 0.000 | ppl 1.000\n",
      "99000it [45:51, 30.19it/s]Train   3: iter 99000 | loss 0.000 | ppl 1.000\n",
      "100000it [46:21, 29.08it/s]Train   3: iter 100000 | loss 0.000 | ppl 1.000\n",
      "Val   3: iter 100000 | loss 0.000 | ppl 1.000\n",
      "100997it [47:02, 28.96it/s]Train   3: iter 101000 | loss 0.000 | ppl 1.000\n",
      "102000it [47:35, 32.97it/s]Train   3: iter 102000 | loss 0.000 | ppl 1.000\n",
      "102999it [48:07, 28.07it/s]Train   3: iter 103000 | loss 0.000 | ppl 1.000\n",
      "104000it [48:38, 33.44it/s]Train   3: iter 104000 | loss 0.000 | ppl 1.000\n",
      "105000it [49:09, 28.81it/s]Train   3: iter 105000 | loss 0.000 | ppl 1.000\n",
      "105999it [49:42, 28.58it/s]Train   3: iter 106000 | loss 0.000 | ppl 1.000\n",
      "106998it [50:17, 29.06it/s]Train   3: iter 107000 | loss 0.000 | ppl 1.000\n",
      "108000it [50:48, 38.40it/s]Train   3: iter 108000 | loss 0.000 | ppl 1.000\n",
      "108999it [51:20, 29.31it/s]Train   3: iter 109000 | loss 0.000 | ppl 1.000\n",
      "109997it [51:52, 35.50it/s]Train   3: iter 110000 | loss 0.000 | ppl 1.000\n",
      "Val   3: iter 110000 | loss 0.000 | ppl 1.000\n",
      "110998it [52:33, 28.04it/s]Train   3: iter 111000 | loss 0.000 | ppl 1.000\n",
      "111998it [53:03, 39.26it/s]Train   3: iter 112000 | loss 0.000 | ppl 1.000\n",
      "112997it [53:33, 30.58it/s]Train   3: iter 113000 | loss 0.000 | ppl 1.000\n",
      "113997it [54:02, 35.99it/s]Train   3: iter 114000 | loss 0.000 | ppl 1.000\n",
      "114999it [54:32, 28.99it/s]Train   3: iter 115000 | loss 0.000 | ppl 1.000\n",
      "115999it [55:01, 36.49it/s]Train   3: iter 116000 | loss 0.000 | ppl 1.000\n",
      "116997it [55:29, 31.13it/s]Train   3: iter 117000 | loss 0.000 | ppl 1.000\n",
      "117998it [55:57, 39.04it/s]Train   3: iter 118000 | loss 0.000 | ppl 1.000\n",
      "118998it [56:23, 38.20it/s]Train   3: iter 119000 | loss 0.000 | ppl 1.000\n",
      "119997it [56:51, 35.71it/s]Train   3: iter 120000 | loss 0.000 | ppl 1.000\n",
      "Val   3: iter 120000 | loss 0.000 | ppl 1.000\n",
      "120998it [57:28, 37.92it/s]Train   3: iter 121000 | loss 0.000 | ppl 1.000\n",
      "121998it [57:57, 31.00it/s]Train   3: iter 122000 | loss 0.000 | ppl 1.000\n",
      "122999it [58:23, 42.38it/s]Train   3: iter 123000 | loss 0.000 | ppl 1.000\n",
      "123417it [58:33, 35.12it/s]\n",
      "0it [00:00, ?it/s]Train   4: iter     0 | loss 0.000 | ppl 1.000\n",
      "Val   4: iter     0 | loss 0.000 | ppl 1.000\n",
      "998it [00:36, 35.99it/s]Train   4: iter  1000 | loss 0.000 | ppl 1.000\n",
      "1999it [01:06, 28.94it/s]Train   4: iter  2000 | loss 0.000 | ppl 1.000\n",
      "3000it [01:36, 30.61it/s]Train   4: iter  3000 | loss 0.000 | ppl 1.000\n",
      "3999it [02:07, 37.64it/s]Train   4: iter  4000 | loss 0.000 | ppl 1.000\n",
      "4998it [02:38, 43.38it/s]Train   4: iter  5000 | loss 0.000 | ppl 1.000\n",
      "5999it [03:06, 36.92it/s]Train   4: iter  6000 | loss 0.000 | ppl 1.000\n",
      "6998it [03:36, 29.25it/s]Train   4: iter  7000 | loss 0.000 | ppl 1.000\n",
      "7997it [04:08, 29.82it/s]Train   4: iter  8000 | loss 0.000 | ppl 1.000\n",
      "8999it [04:40, 30.87it/s]Train   4: iter  9000 | loss 0.000 | ppl 1.000\n",
      "9997it [05:13, 33.18it/s]Train   4: iter 10000 | loss 0.000 | ppl 1.000\n",
      "Val   4: iter 10000 | loss 0.000 | ppl 1.000\n",
      "10996it [05:53, 40.52it/s]Train   4: iter 11000 | loss 0.000 | ppl 1.000\n",
      "11997it [06:19, 40.41it/s]Train   4: iter 12000 | loss 0.000 | ppl 1.000\n",
      "12998it [06:49, 34.35it/s]Train   4: iter 13000 | loss 0.000 | ppl 1.000\n",
      "13998it [07:15, 40.00it/s]Train   4: iter 14000 | loss 0.000 | ppl 1.000\n",
      "15000it [07:40, 45.30it/s]Train   4: iter 15000 | loss 0.000 | ppl 1.000\n",
      "16000it [08:07, 39.30it/s]Train   4: iter 16000 | loss 0.000 | ppl 1.000\n",
      "17000it [08:32, 39.96it/s]Train   4: iter 17000 | loss 0.000 | ppl 1.000\n",
      "17997it [08:58, 39.60it/s]Train   4: iter 18000 | loss 0.000 | ppl 1.000\n",
      "18999it [09:24, 36.96it/s]Train   4: iter 19000 | loss 0.000 | ppl 1.000\n",
      "19999it [09:54, 27.79it/s]Train   4: iter 20000 | loss 0.000 | ppl 1.000\n",
      "Val   4: iter 20000 | loss 0.000 | ppl 1.000\n",
      "21000it [10:33, 34.19it/s]Train   4: iter 21000 | loss 0.000 | ppl 1.000\n",
      "21997it [11:03, 36.05it/s]Train   4: iter 22000 | loss 0.000 | ppl 1.000\n",
      "22998it [11:33, 34.56it/s]Train   4: iter 23000 | loss 0.000 | ppl 1.000\n",
      "23999it [12:03, 34.34it/s]Train   4: iter 24000 | loss 0.000 | ppl 1.000\n",
      "25000it [12:33, 34.66it/s]Train   4: iter 25000 | loss 0.000 | ppl 1.000\n",
      "25998it [13:07, 29.19it/s]Train   4: iter 26000 | loss 0.000 | ppl 1.000\n",
      "26998it [13:40, 32.18it/s]Train   4: iter 27000 | loss 0.000 | ppl 1.000\n",
      "28000it [14:14, 30.27it/s]Train   4: iter 28000 | loss 0.000 | ppl 1.000\n",
      "28999it [14:48, 34.30it/s]Train   4: iter 29000 | loss 0.000 | ppl 1.000\n",
      "30000it [15:20, 28.20it/s]Train   4: iter 30000 | loss 0.000 | ppl 1.000\n",
      "Val   4: iter 30000 | loss 0.000 | ppl 1.000\n",
      "30996it [16:00, 30.36it/s]Train   4: iter 31000 | loss 0.000 | ppl 1.000\n",
      "32000it [16:27, 39.88it/s]Train   4: iter 32000 | loss 0.000 | ppl 1.000\n",
      "32999it [16:52, 41.46it/s]Train   4: iter 33000 | loss 0.000 | ppl 1.000\n",
      "33999it [17:17, 41.00it/s]Train   4: iter 34000 | loss 0.000 | ppl 1.000\n",
      "34997it [17:42, 39.95it/s]Train   4: iter 35000 | loss 0.000 | ppl 1.000\n",
      "35998it [18:07, 38.36it/s]Train   4: iter 36000 | loss 0.000 | ppl 1.000\n",
      "36998it [18:32, 36.61it/s]Train   4: iter 37000 | loss 0.000 | ppl 1.000\n",
      "38000it [18:56, 42.30it/s]Train   4: iter 38000 | loss 0.000 | ppl 1.000\n",
      "38998it [19:21, 39.40it/s]Train   4: iter 39000 | loss 0.000 | ppl 1.000\n",
      "39998it [19:46, 42.44it/s]Train   4: iter 40000 | loss 0.000 | ppl 1.000\n",
      "Val   4: iter 40000 | loss 0.000 | ppl 1.000\n",
      "40997it [20:26, 30.13it/s]Train   4: iter 41000 | loss 0.000 | ppl 1.000\n",
      "41998it [20:57, 38.06it/s]Train   4: iter 42000 | loss 0.000 | ppl 1.000\n",
      "42998it [21:27, 29.94it/s]Train   4: iter 43000 | loss 0.000 | ppl 1.000\n",
      "44000it [21:59, 28.78it/s]Train   4: iter 44000 | loss 0.000 | ppl 1.000\n",
      "45000it [22:31, 30.24it/s]Train   4: iter 45000 | loss 0.000 | ppl 1.000\n",
      "46000it [23:02, 26.71it/s]Train   4: iter 46000 | loss 0.000 | ppl 1.000\n",
      "46997it [23:35, 35.20it/s]Train   4: iter 47000 | loss 0.000 | ppl 1.000\n",
      "47998it [24:07, 35.38it/s]Train   4: iter 48000 | loss 0.000 | ppl 1.000\n",
      "48999it [24:38, 37.38it/s]Train   4: iter 49000 | loss 0.000 | ppl 1.000\n",
      "49999it [25:08, 26.88it/s]Train   4: iter 50000 | loss 0.000 | ppl 1.000\n",
      "Val   4: iter 50000 | loss 0.000 | ppl 1.000\n",
      "51000it [25:48, 34.87it/s]Train   4: iter 51000 | loss 0.000 | ppl 1.000\n",
      "51999it [26:21, 29.76it/s]Train   4: iter 52000 | loss 0.000 | ppl 1.000\n",
      "53000it [26:51, 34.51it/s]Train   4: iter 53000 | loss 0.000 | ppl 1.000\n",
      "54000it [27:23, 29.09it/s]Train   4: iter 54000 | loss 0.000 | ppl 1.000\n",
      "54998it [27:57, 27.87it/s]Train   4: iter 55000 | loss 0.000 | ppl 1.000\n",
      "55999it [28:28, 32.29it/s]Train   4: iter 56000 | loss 0.000 | ppl 1.000\n",
      "56998it [28:58, 29.19it/s]Train   4: iter 57000 | loss 0.000 | ppl 1.000\n",
      "58000it [29:29, 34.98it/s]Train   4: iter 58000 | loss 0.000 | ppl 1.000\n",
      "58996it [30:00, 34.44it/s]Train   4: iter 59000 | loss 0.000 | ppl 1.000\n",
      "59999it [30:30, 28.71it/s]Train   4: iter 60000 | loss 0.000 | ppl 1.000\n",
      "Val   4: iter 60000 | loss 0.000 | ppl 1.000\n",
      "61000it [31:10, 33.37it/s]Train   4: iter 61000 | loss 0.000 | ppl 1.000\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "61999it [31:43, 27.60it/s]Train   4: iter 62000 | loss 0.000 | ppl 1.000\n",
      "62999it [32:15, 36.07it/s]Train   4: iter 63000 | loss 0.000 | ppl 1.000\n",
      "63996it [32:48, 38.60it/s]Train   4: iter 64000 | loss 0.000 | ppl 1.000\n",
      "64997it [33:17, 41.77it/s]Train   4: iter 65000 | loss 0.000 | ppl 1.000\n",
      "65999it [33:42, 42.73it/s]Train   4: iter 66000 | loss 0.000 | ppl 1.000\n",
      "67000it [34:06, 43.74it/s]Train   4: iter 67000 | loss 0.000 | ppl 1.000\n",
      "67998it [34:30, 42.13it/s]Train   4: iter 68000 | loss 0.000 | ppl 1.000\n",
      "68998it [34:55, 40.11it/s]Train   4: iter 69000 | loss 0.000 | ppl 1.000\n",
      "69999it [35:21, 35.83it/s]Train   4: iter 70000 | loss 0.000 | ppl 1.000\n",
      "Val   4: iter 70000 | loss 0.000 | ppl 1.000\n",
      "70996it [35:57, 42.68it/s]Train   4: iter 71000 | loss 0.000 | ppl 1.000\n",
      "72000it [36:24, 32.91it/s]Train   4: iter 72000 | loss 0.000 | ppl 1.000\n",
      "72998it [36:52, 36.18it/s]Train   4: iter 73000 | loss 0.000 | ppl 1.000\n",
      "73997it [37:18, 26.86it/s]Train   4: iter 74000 | loss 0.000 | ppl 1.000\n",
      "75000it [37:46, 37.98it/s]Train   4: iter 75000 | loss 0.000 | ppl 1.000\n",
      "75999it [38:11, 42.22it/s]Train   4: iter 76000 | loss 0.000 | ppl 1.000\n",
      "76997it [38:37, 36.75it/s]Train   4: iter 77000 | loss 0.000 | ppl 1.000\n",
      "77998it [39:01, 41.79it/s]Train   4: iter 78000 | loss 0.000 | ppl 1.000\n",
      "79000it [39:26, 39.72it/s]Train   4: iter 79000 | loss 0.000 | ppl 1.000\n",
      "80000it [39:53, 39.25it/s]Train   4: iter 80000 | loss 0.000 | ppl 1.000\n",
      "Val   4: iter 80000 | loss 0.000 | ppl 1.000\n",
      "81000it [40:29, 36.96it/s]Train   4: iter 81000 | loss 0.000 | ppl 1.000\n",
      "81998it [41:01, 25.42it/s]Train   4: iter 82000 | loss 0.000 | ppl 1.000\n",
      "82998it [41:32, 37.92it/s]Train   4: iter 83000 | loss 0.000 | ppl 1.000\n",
      "83999it [42:01, 33.68it/s]Train   4: iter 84000 | loss 0.000 | ppl 1.000\n",
      "84999it [42:33, 29.64it/s]Train   4: iter 85000 | loss 0.000 | ppl 1.000\n",
      "86000it [42:59, 39.53it/s]Train   4: iter 86000 | loss 0.000 | ppl 1.000\n",
      "87000it [43:25, 40.20it/s]Train   4: iter 87000 | loss 0.000 | ppl 1.000\n",
      "87999it [43:50, 35.97it/s]Train   4: iter 88000 | loss 0.000 | ppl 1.000\n",
      "88996it [44:16, 40.71it/s]Train   4: iter 89000 | loss 0.000 | ppl 1.000\n",
      "89999it [44:41, 42.16it/s]Train   4: iter 90000 | loss 0.000 | ppl 1.000\n",
      "Val   4: iter 90000 | loss 0.000 | ppl 1.000\n",
      "[!] saving model...\n",
      "91000it [45:13, 39.25it/s]Train   4: iter 91000 | loss 0.000 | ppl 1.000\n",
      "92000it [45:39, 41.55it/s]Train   4: iter 92000 | loss 0.000 | ppl 1.000\n",
      "92997it [46:02, 45.72it/s]Train   4: iter 93000 | loss 0.000 | ppl 1.000\n",
      "93997it [46:25, 39.82it/s]Train   4: iter 94000 | loss 0.000 | ppl 1.000\n",
      "94998it [46:50, 36.95it/s]Train   4: iter 95000 | loss 0.000 | ppl 1.000\n",
      "95999it [47:14, 42.34it/s]Train   4: iter 96000 | loss 0.000 | ppl 1.000\n",
      "96996it [47:38, 40.11it/s]Train   4: iter 97000 | loss 0.000 | ppl 1.000\n",
      "98000it [48:02, 42.66it/s]Train   4: iter 98000 | loss 0.000 | ppl 1.000\n",
      "99000it [48:26, 42.78it/s]Train   4: iter 99000 | loss 0.000 | ppl 1.000\n",
      "99997it [48:53, 33.32it/s]Train   4: iter 100000 | loss 0.000 | ppl 1.000\n",
      "Val   4: iter 100000 | loss 0.000 | ppl 1.000\n",
      "100998it [49:31, 29.07it/s]Train   4: iter 101000 | loss 0.000 | ppl 1.000\n",
      "102000it [49:57, 41.91it/s]Train   4: iter 102000 | loss 0.000 | ppl 1.000\n",
      "102998it [50:22, 33.79it/s]Train   4: iter 103000 | loss 0.000 | ppl 1.000\n",
      "103999it [50:50, 35.08it/s]Train   4: iter 104000 | loss 0.000 | ppl 1.000\n",
      "104999it [51:14, 42.94it/s]Train   4: iter 105000 | loss 0.000 | ppl 1.000\n",
      "105997it [51:40, 33.13it/s]Train   4: iter 106000 | loss 0.000 | ppl 1.000\n",
      "107000it [52:10, 26.43it/s]Train   4: iter 107000 | loss 0.000 | ppl 1.000\n",
      "107999it [52:38, 32.94it/s]Train   4: iter 108000 | loss 0.000 | ppl 1.000\n",
      "108998it [53:06, 38.86it/s]Train   4: iter 109000 | loss 0.000 | ppl 1.000\n",
      "110000it [53:34, 32.70it/s]Train   4: iter 110000 | loss 0.000 | ppl 1.000\n",
      "Val   4: iter 110000 | loss 0.000 | ppl 1.000\n",
      "110484it [53:55, 42.45it/s]"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\r\n",
    "from pretrain_trfm import TrfmSeq2seq\r\n",
    "from pretrain_rnn import RNNSeq2Seq\r\n",
    "#from bert import BERT\r\n",
    "from build_vocab import WordVocab\r\n",
    "from utils import split\r\n",
    "\r\n",
    "pad_index = 0\r\n",
    "unk_index = 1\r\n",
    "eos_index = 2\r\n",
    "sos_index = 3\r\n",
    "mask_index = 4\r\n",
    "\r\n",
    "vocab = WordVocab.load_vocab('/home/yogesh/data/dnndr2/processedData/vocab_24.pkl')\r\n",
    "\r\n",
    "trfm = TrfmSeq2seq(len(vocab), 256, len(vocab), 3)\r\n",
    "model = nn.DataParallel(model)\r\n",
    "trfm.load_state_dict(torch.load('/home/yogesh/data/dnndr2/pretrained_ST/trfm_12_23000.pkl'))\r\n",
    "trfm.eval()\r\n",
    "print('Total parameters:', sum(p.numel() for p in trfm.parameters()))"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for TrfmSeq2seq:\n\tUnexpected key(s) in state_dict: \"trfm.encoder.layers.3.self_attn.in_proj_weight\", \"trfm.encoder.layers.3.self_attn.in_proj_bias\", \"trfm.encoder.layers.3.self_attn.out_proj.weight\", \"trfm.encoder.layers.3.self_attn.out_proj.bias\", \"trfm.encoder.layers.3.linear1.weight\", \"trfm.encoder.layers.3.linear1.bias\", \"trfm.encoder.layers.3.linear2.weight\", \"trfm.encoder.layers.3.linear2.bias\", \"trfm.encoder.layers.3.norm1.weight\", \"trfm.encoder.layers.3.norm1.bias\", \"trfm.encoder.layers.3.norm2.weight\", \"trfm.encoder.layers.3.norm2.bias\", \"trfm.decoder.layers.3.self_attn.in_proj_weight\", \"trfm.decoder.layers.3.self_attn.in_proj_bias\", \"trfm.decoder.layers.3.self_attn.out_proj.weight\", \"trfm.decoder.layers.3.self_attn.out_proj.bias\", \"trfm.decoder.layers.3.multihead_attn.in_proj_weight\", \"trfm.decoder.layers.3.multihead_attn.in_proj_bias\", \"trfm.decoder.layers.3.multihead_attn.out_proj.weight\", \"trfm.decoder.layers.3.multihead_attn.out_proj.bias\", \"trfm.decoder.layers.3.linear1.weight\", \"trfm.decoder.layers.3.linear1.bias\", \"trfm.decoder.layers.3.linear2.weight\", \"trfm.decoder.layers.3.linear2.bias\", \"trfm.decoder.layers.3.norm1.weight\", \"trfm.decoder.layers.3.norm1.bias\", \"trfm.decoder.layers.3.norm2.weight\", \"trfm.decoder.layers.3.norm2.bias\", \"trfm.decoder.layers.3.norm3.weight\", \"trfm.decoder.layers.3.norm3.bias\". ",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_220081/1489857452.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mtrfm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrfmSeq2seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtrfm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/yogesh/data/dnndr2/pretrained_ST/trfm_12_23000.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mtrfm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Total parameters:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrfm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dnndr2/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1407\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for TrfmSeq2seq:\n\tUnexpected key(s) in state_dict: \"trfm.encoder.layers.3.self_attn.in_proj_weight\", \"trfm.encoder.layers.3.self_attn.in_proj_bias\", \"trfm.encoder.layers.3.self_attn.out_proj.weight\", \"trfm.encoder.layers.3.self_attn.out_proj.bias\", \"trfm.encoder.layers.3.linear1.weight\", \"trfm.encoder.layers.3.linear1.bias\", \"trfm.encoder.layers.3.linear2.weight\", \"trfm.encoder.layers.3.linear2.bias\", \"trfm.encoder.layers.3.norm1.weight\", \"trfm.encoder.layers.3.norm1.bias\", \"trfm.encoder.layers.3.norm2.weight\", \"trfm.encoder.layers.3.norm2.bias\", \"trfm.decoder.layers.3.self_attn.in_proj_weight\", \"trfm.decoder.layers.3.self_attn.in_proj_bias\", \"trfm.decoder.layers.3.self_attn.out_proj.weight\", \"trfm.decoder.layers.3.self_attn.out_proj.bias\", \"trfm.decoder.layers.3.multihead_attn.in_proj_weight\", \"trfm.decoder.layers.3.multihead_attn.in_proj_bias\", \"trfm.decoder.layers.3.multihead_attn.out_proj.weight\", \"trfm.decoder.layers.3.multihead_attn.out_proj.bias\", \"trfm.decoder.layers.3.linear1.weight\", \"trfm.decoder.layers.3.linear1.bias\", \"trfm.decoder.layers.3.linear2.weight\", \"trfm.decoder.layers.3.linear2.bias\", \"trfm.decoder.layers.3.norm1.weight\", \"trfm.decoder.layers.3.norm1.bias\", \"trfm.decoder.layers.3.norm2.weight\", \"trfm.decoder.layers.3.norm2.bias\", \"trfm.decoder.layers.3.norm3.weight\", \"trfm.decoder.layers.3.norm3.bias\". "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vocab."
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "load_vocab() missing 1 required positional argument: 'vocab_path'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_229018/3000802496.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: load_vocab() missing 1 required positional argument: 'vocab_path'"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('kddcup': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "interpreter": {
   "hash": "29c3233f0e21dae1881ae37bb6fa93af8f6df5c4d3ae143db1ced74631538065"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}